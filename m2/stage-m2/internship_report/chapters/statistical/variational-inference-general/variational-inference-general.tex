\documentclass[../../main.tex]{subfiles} % Allows individual compilation
\graphicspath{{\subfix{./images/}}}

\begin{document}
This chapter presents two distinct approaches to finding communities on a 
graph. First, a statistical approach is developed, where a stochastic block model 
is assumed to have generated the graph observed and then a particular 
variational method is employed to estimate this model's parameters and infer 
the communities. Then, an alternative approach based on analyzing the 
spectrum of graph operators is introduced. 

\section{Statistical approach}
In this section, a framework for \textit{statistical} community detection is laid 
down. It consists in assuming a SBM model for the observed graph, then 
estimating the parameters of the model by approximately maximizing the 
likelihood using an algorithm similar to the classical EM, called the 
\textit{variational EM algorithm}. In this process, the resulting 
\textit{variational 
parameters}, that control the approximation of the likelihood, assign 
for each 
node in the graph a vector describing the probability of it belonging 
to each 
community; a community estimation can be obtained by taking the 
index of the 
maximal entry of this vector, for each node. Notice that this is one 
particular 
instance of statistical approach; others can be developed by choosing 
different 
models (such as latent block models, geometric block models, etc) or 
different 
estimation procedures (such as stochastic EM, variational Bayes, 
Gibbs 
sampling, and so on).

\subsection{The stochastic block model}
Part of the statistical procedure is to assume a model for the observations at 
hand. This subsection defines a model which is popularly assumed in the 
context of community detection on graphs, and which will be used in the 
analysis that follows.

\subsubsection{The general SBM}
One canonical probabilistic model for graphs with community structure is 
called 
the \textit{Stochastic Block Model}, or SBM for short. For a lengthier 
discussion 
on the origins and variants of this model, refer to 
\cite{abbe_community_2017}. In the definition that follows, \(\mathcal M(n, p)\) 
and \(\text{Ber}(p)\) denote the multinomial and Bernoulli distribution, 
respectively.

\begin{definition}[Stochastic Block Model] \label{def:sbm}
	Let \(n \in \mathbb N\), \(k \in \mathbb N\), \(\pi = (\pi_1, \dots, 
	\pi_k)\) be a probability vector on \([k] := \{1, \dots, k\}\) and  
	\(\Gamma\) be a \(k \times k\) symmetric matrix with entries \(\gamma_{ij} 
	\in [0, 1]\). A pair \((Z, G)\) is said to be \textit{drawn under a 
		\SBM{n}{\pi}{\Gamma}} if 
	\begin{itemize}
		\item \(Z = (Z_1, \dots, Z_n)\) is an \(n\)-tuple of \(\mathbb 
		N^k\)-valued random variables, \(Z_i \sim \mathcal M (1, \pi)\),
		\item \(G\) is a simple graph with \(n\) vertices whose symmetric 
		adjacency matrix has zero diagonal and for \(j > i\), \(A_{ij} \vert Z \sim 
		\text{Ber}(\gamma_{Z_i, Z_j})\), the lower triangular part being 
		completed by symmetry.
	\end{itemize}
\end{definition}

\begin{figure}[ht]
	\centering
	\ifthenelse{\boolean{addtikz}}{
		\subfile{./images/tikz/sbm-obs/sbm-obs.tex}
	}{
		\includegraphics{example-image-a}
	}
	\caption{An example of an SBM graph with assortative communities}
	\label{fig:sbm-example}
\end{figure}

\begin{remark}
	The quantity \(n\) should be thought of as being the number of nodes in 
	\(G\), \(k\) should be thought of as being the number of communities in 
	\(G\), \(\pi\) should be thought of as being a prior on the community 
	assignments \(Z\), and \(\Gamma\) should be thought of as a matrix of 
	intra-cluster and inter-cluster connectivities. The random variables of the 
	model are the \(n\) community assignments \(Z\) and the \(\binom{n}{2}\) 
	entries \(A_{ij}\) of the adjacency matrix.
\end{remark}

\begin{remark}
	Although each community assignment \(Z_i\) is a vector, the same \(Z_i\) 
	can be used to denote the \textit{number} of the community that node \(i\) 
	is assigned to, to make notation lighter.
\end{remark}

It is important to emphasize that although intuition frequently refers to the 
assortative case, such as in Figure \ref{fig:sbm-example}, the SBM is versatile 
and can reproduce many other characteristics of graphs with communities. For 
instance, the SBM can generate bipartite graphs as a model for the example 
given in the introduction, where couples dance in a party; it can also generate 
graphs with ``stars'', and reproduce the ``core-periphery'' phenomenon. See 
Figure \ref{fig:sbm-versatile}.

\begin{figure}
	\centering
	\setlength\tabcolsep{4ex}
	\makebox[\textwidth][c]{
		\begin{tabular}{cc}
			\begin{subfigure}{.45\textwidth}
				\centering
				\ifthenelse{\boolean{addtikz}}{
					\resizebox{!}{\linewidth}{
						\subfile{./images/tikz/bipartite-sbm/bipartite-sbm.tex}
					}
				}{
					\includegraphics[width=\linewidth]{example-image-a}
				}
				\caption{}
				\label{fig:bipartite-sbm}
			\end{subfigure}
			&
			\begin{subfigure}{.45\textwidth}
				\centering
				\ifthenelse{\boolean{addtikz}}{
					\resizebox{!}{\linewidth}{
						\subfile{./images/tikz/star-sbm/star-sbm.tex}
					}
				}{
					\includegraphics[width=\linewidth]{example-image-a}
				}
				\caption{}
				\label{fig:star-sbm}
			\end{subfigure}
			\\
			\multicolumn{2}{c}{
				\begin{subfigure}{.5\textwidth}
					\centering
					\ifthenelse{\boolean{addtikz}}{
						\resizebox{!}{\linewidth}{
							
\subfile{./images/tikz/core-periphery-sbm/core-periphery-sbm.tex}
						}
					}{
						\includegraphics[width=\linewidth]{example-image-a}
					}
					\caption{}
					\label{fig:core-periphery-sbm}
				\end{subfigure}
			}
		\end{tabular}
	}
	\caption{The SBM is versatile and can give rise to different features, such 
		as (a) bipartite structures, (b) star structures, (c) core-periphery 
		structures.}
	\label{fig:sbm-versatile}
\end{figure}

\subsubsection{The symmetric SBM}
Even though the SBM is a simple and intuitive model for graphs with 
communities, the calculations associated with it can already yield long 
expressions and present subtleties. For this reason, it is desirable to 
have a yet simpler version of the SBM where one can test intuitions and 
perform 
preliminary calculations. The symmetric SBM is precisely such a model. 
\begin{definition}[Symmetric SBM]
	The pair \((X, G)\) is drawn from \SSBM{n}{k}{p}{q} if it is drawn from an 
	SBM model with \(\pi = \frac{1}{k} \ones[k]\) and \(\Gamma\) taking values 
	\(p\) on the diagonal and \(q\) outside the diagonal.
\end{definition}

\subsection{Model likelihood}
After choosing a model for one's observations, the next step in the statistical 
procedure is to estimate its parameters. A popular estimation procedure is to 
maximize \textit{likelihood} of the parameters. This subsection introduces 
the SBM likelihoods, while also explaining how to deal with the presence of 
latent variables.

\subsubsection{Complete model likelihood}
Given the observation of a graph \(G\), represented by its adjacency matrix 
\(A\), and fixing community assignments \(Z\) for it, one can calculate for 
each 
choice of model parameters \(\theta \coloneqq (\pi, \Gamma)\) the 
probability \(p_{\theta} (A, Z)\) of observing \(A, Z\) under the model 
\(\text{SBM}(n, \pi, 
\Gamma)\). 
The function associating parameters \(\theta \to p_{\theta} (A, 
Z)\) is called the \textit{complete likelihood} for \((A, Z)\). It is ``complete'' in 
the sense that it assumes knowledge of \(Z\), which is latent (not observed in 
practice). It can be explicitly written as
\begin{equation} \label{eq:complete-likelihood}
	p_{\theta}(A, Z) = \prod_{i=1}^{n} \pi_{Z_i} \prod_{\substack{i=1 \\ 
	j>i}}^n 
	\gamma_{Z_i 
		Z_j}^{A_{ij}} (1 - \gamma_{Z_i Z_j})^{1 - A_{ij}}.
\end{equation}

\begin{remark}
	Equation \eqref{eq:complete-likelihood} is an example of the abuse of 
	notation described in the remarks below Definition \ref{def:sbm}, 
	since here 
	\(Z_i\) denotes the \textit{number} of the community associated to 
	node \(i\).
\end{remark}

\subsubsection{Observed model likelihood}
In order to have a likelihood associated to an observation, one needs to 
marginalize the latent variables present in Equation 
\eqref{eq:complete-likelihood}. That is, if \(n\) is the number of 
nodes in the 
graph, and \(k\) is the number of communities, then
\begin{equation} \label{eq:observed-likelihood}
	p_\theta(A) \coloneqq \sum_{Z \in \{1, \dots, k\}^n} p_\theta(A, Z).
\end{equation}

However, in practice the calculation of this sum is intractable. First 
notice that 
there is a complex dependency structure between the \(Z_i\) given 
\(A\). To see 
this, consider the case of \(k = 2\) communities and without loss of 
generality 
enumerate any pair of nodes as nodes \(1\) and \(2\). The edge 
connecting 
them is represented in the adjacency matrix by \(A_{12}\). The 
graphical model 
of this situation is shown in Figure \ref{fig:v-structure}. The joint 
probability 
distribution of \(Z_1, Z_2 \vert A\) writes
\begin{equation}
	p(Z_1, Z_2 \vert A_{12}) = \frac{p(Z_1, Z_2, A_{12})}{p(A_{12})} = 
	\frac{p(A_{12} \vert Z_1, Z_2) p(Z_1) p(Z_2)}{p(A_{12})}.
	\label{eq:dependence-structure}
\end{equation}
which does not factorizes as \(p(Z_1 \vert A_{12}) p(Z_2 \vert 
A_{12})\), 
showing the dependence of \(Z_1\) on \(Z_2\) (and vice-versa) 
conditionally on 
\(A_{12}\). As a consequence, the sum cannot be simplified by 
writing \(p(A, Z) 
= p(Z \vert A) p(Z)\). Moreover, the sum has a number of terms 
which is exponential on the number of nodes \(n\), making its computation 
infeasible for many graphs appearing in applications.
\begin{figure}
	\centering
	\subfile{./images/tikz/v-structure/v-structure}
	\caption{Graphical model expressing the dependence structure 
	between 
	latent variables \(Z_1, Z_2\) and edge \(A_{12}\); this pattern is 
	commonly 
	called a ``v-structure'' \cite{koller2009probabilistic}}
	\label{fig:v-structure}
\end{figure}

Therefore, for most practical applications it will be necessary to approximate 
this observed likelihood in order to perform statistical estimation and 
inference on SBMs.

\subsection{Variational decomposition and mean field approximation}
The likelihood in Equation \eqref{eq:observed-likelihood} is complex to deal 
with, since it has a number of terms exponential in \(n\) and also because it 
can have multiple local optima. Therefore, in most cases approximations are 
needed in order to work with this model. A common one is the variational 
approximation to the likelihood. This is still complicated in all its generality, 
so a second ``mean field'' approximation is used on top of the first 
variational one. This consists in searching the solution to the variational 
approximation amidst factorizable distributions. These approximations will 
deal with the problem of having an exponential number of complicated 
terms, but unfortunately the problem of multiple local optima will still 
remain.

\subsubsection{Deriving the variational 
decomposition.}
Let \(Z\) denote the vector of latent variables, \(A\) an observation, 
and \(\theta 
\coloneqq (\pi, \Gamma)\) the parameters of an SBM. The following 
\textit{variational decomposition} leads to an useful approximation 
to the 
likelihood.

\begin{definition}
	For any two probability densities \(p, q\) such that \(q(z) = 0 \implies 
	p(z) = 0\),
	\begin{equation}
		\KL (q \Vert p) \coloneqq -
		\int_{Z} \log \left( \frac{p (z)}{q(z)}
		\right) q(z) \, dz
	\end{equation}
	is called the \textit{Kullback-Leibler divergence} from \(p\) to 
	\(q\).
\end{definition}

\begin{theorem} \label{thm:var-decomp}
	For any probability mass function \(q\) over \(\{1, \dots, k\}^n\), the 
	observed 
	likelihood can be decomposed as
	\begin{equation}
		\log p_\theta (A) = F (q, \theta) + \KL (q \Vert p_\theta( Z \vert 
		A)),
		\label{eq:variational-decomposition}
	\end{equation}
	where
	\begin{equation}
		F (q, \theta) \coloneqq \int_{\{1, \dots, k\}^n} \log \left(
		\frac{p_\theta \left( Z, A\right)}{q(Z)} \right) q(Z) \, dZ
	\end{equation}
	is called the \textit{evidence lower bound}, or ELBO for short.
\end{theorem}

For the proof, see Section \ref{app:proof-var-decomp}. Equation 
\eqref{eq:variational-decomposition} forms the basis of the classical 
EM algorithm for estimating \(\theta\), where one performs alternate 
minimization of the KL term and subsequent maximization of the term 
\(F \left( q, \theta \right)\). 
A classical result proves that the KL is always positive. Therefore, the ELBO 
is indeed a lower bound for the log likelihood being decomposed. 
\marginnote{In the mean-field case, the ELBO is also sometimes 
called the ``free energy'', due to its relevance in the physics 
literature.}[-15ex] 
Given the independence of the left hand side with respect to $q$, observe that 
maximizing the ELBO amounts to minimizing the KL term, and this can be 
done by 
setting \(q = p_\theta(Z \vert A)\). However, in the case of the SBM, 
calculating expectations with respect to this conditional probability is itself 
intractable, due to its complex dependency structure (Equation 
\eqref{eq:dependence-structure}), so this step of EM must be performed 
differently.

\subsubsection{Mean field approximation.} A common strategy used to 
deal 
with the problem of having an untractable solution \(q\) to the variational 
approximation is called the ``mean field approximation''. 
\marginnote{There 
are other ``correlated'' mean field approximations, see 
\cite{mezard2009information}.}[-2ex] It consists in trying to find a \(q\) 
distribution maximizing the ELBO constrained to a family of tractable 
distributions. One possible choice for tractability is to consider factorizable 
distributions, that is, distributions of the form
\begin{equation*}
	q \left( Z \right) = \prod_{i=1}^{n} q_i \left( Z_i \right).
\end{equation*}

\subsection{Variational estimation of the SBM} 
\label{sec:var-estimation-sbm}
\subsubsection{The ELBO in the SBM case}
In the case of the SBM, each factor in the mean field approximation must be 
multinomial distribution, and they differ only by their parameters, that is,
\begin{equation}
	q \left( Z \right) = \prod_{i=1}^n m \left( Z_i; \tau_i \right),
	\label{eq:mean-field-approx}
\end{equation}
where \(m(z, \tau_i) \propto \prod_{j=1}^{k} \tau_{ij}^{z_j}\) is the 
probability mass function of a multinomial
distribution with parameter \(\tau_i\). Notice that \(\tau_i \in [0, 1]^k, 
\sum_j 
\tau_{ij} = 1\). They can be arranged as rows in a single matrix \(\tau\), and 
will collectivelly be called the \textit{variational parameters}, as they control 
the 
distribution being used to approximate the posterior in the variational 
family. It 
is then possible to find an explicit form for the ELBO of an observation 
assuming the SBM model.

\begin{proposition} \label{prop:mf-elbo}
	Given an adjacency matrix \(A\) and assuming an 
	\SBM{n}{\pi}{\Gamma}, the 
	mean-field ELBO is given by
	\begin{dmath}
		F_A (\tau, \theta) = \sum_{i=1}^n \sum_{k=1}^K \left[\tau_{ik}
		\log{\frac{\pi_k}{\tau_{ik}}} \\ + \frac{1}{2} \sum_{j=1}^n \sum_{l=1}^K
		\tau_{ik} \tau_{jl} \left( A_{ij} \log{\gamma_{kl}} + \left( 1 -
		\delta_{ij} - A_{ij}
		\right) \log{\left( 1 - \gamma_{kl} \right)} \right) \right].
		\label{eq:mf-elbo}
	\end{dmath}
\end{proposition}

For the proof, see Section \ref{sec:proof-mf-elbo}. In practice, the 
dependence on \(A\) will be ommitted and the ELBO will be denoted 
solely by \(F\).

\subsubsection{The variational EM algorithm}
Traditionally, optimization under missing data is numerically done via the EM 
algorithm. Maximizing the ELBO in models with latent variables is a particular 
instance of this situation. In this case, since there is the extra step of 
approximating the \(q\) distribution within the mean-field variational family, 
the algorithm is called the \textit{variational EM}, Algorithm \ref{alg:vem}.

\begin{algorithm}
	\caption{Variational EM}
	\label{alg:vem}
	\begin{algorithmic}
		\Require Adjacency matrix \(A\), 
		tolerance \(\texttt{tol}\)
		\State Randomly initialize 
		\(\theta_0 = (\pi_0, 
		\Gamma_0)\)
		\State Randomly initialize \(\tau_0\)
		\State Initialize \(\texttt{variation} \gets 1\)
		\While{\(\texttt{variation} > 
			\texttt{tol}\)}
		\State \(\tau_{t+1} \gets 
		\argmax_\tau F(q_\tau, \theta_{t}) \quad\)(E step)
		\State \( \theta_{t+1} \gets 
	    \argmax_\theta \mathbb 
		E_{q_{\tau_{t+1}}} [\log 
		p_{\theta_{t}} 
		(x, z)] \quad\)(M step)
		\State \(\texttt{variation} \gets 
		F(\theta_{t+1}, \tau_{t+1}) - 
		F(\theta_{t}, \tau_{t})\)
		\EndWhile
		\State Return final parameters \(\theta_f\) and \(\tau_f\)
	\end{algorithmic}
\end{algorithm}

This algorithm is monotone, in the sense that each step always 
makes the 
likelihood increase; see Section \ref{app:em-monotone}. Therefore, 
when the 
likelihood is bounded, the algorithm will converge to some local 
maximum. In 
practice, one performs these steps iteratively until the variation of the ELBO 
becomes negligible.

It is possible to explicitly describe these steps. The E step can be calculated 
by solving the fixed point relation
\begin{equation}
	\hat \tau_{ik} \propto \pi_k \prod_{\substack{j > i \\ l = 1, \dots, K}}
	\left( \gamma_{kl}^{A_{ij}} \left( 1 - \gamma_{kl} \right)^{\left( 1 -
		A_{ij}
		\right)} \right)^{\tau_{jl}}.
	\label{eq:fixed-point-tau}
\end{equation}
The M step can be calculated directly by
\begin{equation} \label{eq:pi-hat}
	\hat \pi_k = \frac{1}{n} \sum_{i=1}^n \tau_{ik}.
\end{equation}
and
\begin{equation} \label{eq:gamma-hat}
	\hat \gamma_{kl} = \frac{\sum_{i=1}^n \sum_{j=1}^n \tau_{ik}
		\tau_{jl} A_{ij}}{\sum_{i=1}^n \sum_{j=1}^n \tau_{ik} \tau_{jl} \left(
		1 - \delta_{ij} \right)}.
\end{equation}
For a proof of these statements, see Section \ref{proof:em-steps}.

\end{document}
