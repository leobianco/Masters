\documentclass[../../main.tex]{subfiles} % Allows individual compilation
\graphicspath{{\subfix{./images/}}}

\begin{document}

\section{The case of two communities*}\label{sec:two-communities}

\subsection{Rewriting the ELBO}
Suppose now that \(k=2\). Then, for any node of index \(i\), \(\tau_{i2} = 1 -
\tau_{i1}\). Thus one can work only with the first component
\(\tau_{i1}\), which will be denoted from now on by \(\tau_i\). A similar logic
applies to \(\pi\) since \(\pi_2 = 1 - \pi_1\), thus subsequently work only with
\(\pi_1\) which will be denoted simply \(\pi\). The ELBO from \ref{eq:mf-elbo} 
writes
\begin{dmath*}
	\mathcal L_{(k=2)} = \sum_{i=1}^n \tau_i \log{\frac{\pi}{\tau_i}} + \left(
	1 - \tau_i \right) \log{\left( \frac{1 - \pi}{1 - \tau_i} \right)}
	% term
	+ \frac{1}{2} \sum_{j \neq i} A_{ij} \left( \tau_i \tau_j
	\log{\gamma_{11}} + \tau_i \left( 1 - \tau_j \right)
	\log{\gamma_{12}} + \left( 1 - \tau_i \right)\tau_j \log{\gamma_{21}} + 
	\left(
	1 - \tau_i \right) \left( 1 - \tau_j \right) \log{\gamma_{22}} \right) 
	% term
	+ \frac{1}{2} \sum_{j \neq i} \left( 1 - A_{ij} \right) \left( \tau_i 
	\tau_j 
	\log{\left( 1 - \gamma_{11} \right)} + \tau_i \left( 1 - \tau_j \right) 
	\log{\left( 1 - \gamma_{12} \right)} + \left( 1 - \tau_i \right) \tau_j 
	\log{\left( 1 - \gamma_{21} \right)} + \left(1 - \tau_i \right) 
	\left( 1 - \tau_j \right) \log{\left( 1 - \gamma_{22}\right)}\right).
\end{dmath*}
This expression can be grouped differently:
\begin{dmath*}
	\mathcal L_{(k=2)} = \sum_{i=1}^n \tau_i \log{\frac{\pi}{\tau_i}} + \left(
	1 - \tau_i \right) \log{\left( \frac{1 - \pi}{1 - \tau_i} \right)}
	% term
	+ \frac{1}{2} \sum_{j \neq i} \tau_i \tau_j \left( A_{ij}
	\log{\gamma_{11}} + \left( 1 - A_{ij} \right) \log{\left( 1 - \gamma_{11}
		\right)} \right)
	% term
	+ \sum_{j \neq i} \tau_i \left( 1 - \tau_j \right) \left( A_{ij}
	\log{\gamma_{12}} + \left( 1 - A_{ij} \right) \log{\left( 1 
		- \gamma_{12} \right)} \right)
	% term
	+ \frac{1}{2} \sum_{j \neq i} \left( 1 - \tau_i \right) \left( 1 -
	\tau_j \right) \left( A_{ij}
	\log{\gamma_{22}} + \left( 1 - A_{ij} \right) \log{\left( 1 - \gamma_{22}
		\right)} \right).
\end{dmath*}
To make things simpler, write this in matrix notation. Let
\(\mathbf{1}_n \coloneqq (1, \dots, 1)\) be a vector of dimension \(n\), 
\(I_n\) be the identity matrix of size \(n \times n\), and \(J \coloneqq 
\mathbf{1}_n \mathbf{1}_n^t - I_n \) be the matrix with zeros on the diagonal 
and ones everywhere else. The previous expression for the ELBO becomes
\begin{dmath} \label{eq:elbo-two-comm}
	\mathcal L_{(k=2)} = \sum_{i=1}^n \tau_i \log{\frac{\pi}{\tau_i}} + \left(
	1 - \tau_i \right) \log{\left( \frac{1 - \pi}{1 - \tau_i} \right)}
	% term
	+ \frac{1}{2} \left( \tau^t A \tau \log{\gamma_{11}} + \tau^t \left(
	J - A \right) \tau \log{\left( 1 - \gamma_{11} \right)} \right)
	% term
	+ \left( \tau^t A \left( \mathbf{1}_n - \tau \right) \log{\gamma_{12}} 
	+ \tau^t \left( J - A \right) \left( \mathbf{1}_n - \tau \right) 
	\log{\left( 1 - \gamma_{12} \right)} \right)
	% term 
	+ \frac{1}{2} \left( \left( \mathbf{1}_n - \tau \right)^t A \left(
	\mathbf{1}_n - \tau \right) \log{\gamma_{22}} + \left( \mathbf{1}_n - 
	\tau \right)^t \left( J - A \right) \left( \mathbf{1}_n - \tau \right) 
	\log{\left( 1 - \gamma_{22} \right)} \right).
\end{dmath}

\subsection{The \(\Phi\) function}
When one observes a graph from an SBM, it is typically not the case that any 
parameter of the model is known, thus it is natural to consider the function 
\(\Phi (\tau) \coloneqq \sup_{\pi, \gamma} \mathcal L (\tau; \pi, \gamma)\). It 
is then natural to consider its empirical version by substituting the 
parameters by their estimators found in equations \ref{eq:pi-hat} and 
\ref{eq:gamma-hat}. Notice that in the binary case, the 
expression in \ref{eq:pi-hat} becomes
\begin{equation}
	\hat \pi = \frac{\ones^t \tau}{n},
	\label{eq:pi-hat-two}
\end{equation}
while the expression in \ref{eq:gamma-hat} becomes
\begin{align}
	\hat \gamma_{11} &= \frac{\tau^t A \tau}{\tau^t J \tau}, \\
	\hat \gamma_{12} = \hat \gamma_{21} &= \frac{\left( \ones - \tau \right)^t 
	A \tau}{\left(
		\ones - \tau \right)^t J \tau}, \\
	\hat \gamma_{22} &= \frac{\left( \ones - \tau \right)^t A \left( \ones -
		\tau
		\right)}{\left( \ones - \tau \right)^t J \left( \ones - \tau \right)}.
\end{align}
Notice also that the equation \(\hat \gamma_{21} = \hat \gamma_{12}\) comes 
from the symmetry of \(A\) and \(J\). Substituting these estimators in the
expression for the ELBO in order to find an expression for \(\hat \Phi (\tau) 
\), one obtains the following equation after some straightforward calculations:
\begin{dmath}
	\hat \Phi \left( \tau \right) = \sum_{i=1}^n H \left( \tau_i \right) - n
	H \left( \frac{\ones^t \tau}{n} \right) \\
	% term
	- \frac{\tau^t J \tau}{2} H \left( \frac{\tau^t A \tau}{\tau^t J \tau}
	\right) \\
	% term
	- \tau^t J \left( \ones - \tau \right) H \left( \frac{\tau^t A \left(
		\ones - \tau \right)}{\tau^t J \left( \ones - \tau \right)} \right)\\
	% term
	- \frac{\left( \ones - \tau \right)^t J \left( \ones - \tau \right)}{2}
	H \left( \frac{\left( \ones - \tau \right)^t A \left( \ones - \tau \right)}
	{\left( \ones - \tau \right)^t J \left( \ones - \tau \right)}\right),
	\label{eq:phi}
\end{dmath}
where \(H(x) \coloneqq -x \log{x} - (1 - x) \log{(1-x)}\) is the entropy of a
Bernoulli. Notice that for \(\tau\) lying on the corners of the hypercube this 
corresponds to a ``sure'' assignment of the nodes of the graph to one of the 
two communities. Of course, trying to optimize such a function on the corners 
of the cube is NP-hard. To simplify this expression, introduce the notations
	\begin{align} \label{eq:combinatorial-notation}
		% First line
		E_\tau &\coloneqq \frac{\tau^t A \tau}{2} & E_{\bar\tau} &\coloneqq 
		\frac{\bar\tau^t A \bar\tau}{2} & E_M &\coloneqq \tau^t A \bar\tau \\
		% Second line
		C_\tau &\coloneqq \frac{\tau^t J \tau}{2} & C_{\bar\tau} &\coloneqq 
		\frac{\bar\tau^t J \bar\tau}{2} & C_M &\coloneqq \tau^t J \bar\tau.
	\end{align}
This is motivated by the fact that in the particular case of \(\tau\) lying on 
the vertices of the cube these quantities equal the number of edges in the 
community determined by \(\tau\) (the nodes \(i\) such that \(\tau_i = 1\)) and 
the number of edges that there would be in the complete graph determined by 
these same nodes (likewise for \(\bar\tau \coloneqq \ones - \tau\)). The edges 
and would-be edges between different communities are included in this notation 
by dropping the \(1/2\) factor. A last (simple) piece of notation is \(n_\tau 
\coloneqq \ones^t \tau\), the number of nodes in the community of nodes with 
\(\tau_i = 1\). Using these notations, the objective in \ref{eq:phi} can be 
more elegantly expressed as
\begin{dmath}
	-\frac{\hat \Phi \left( \tau \right)}{C} =
	% term
	-\frac{1}{C}\sum_{i=1}^n H \left( \tau_i \right) 
	% term
	+ \frac{n}{C} H \left( \frac{n_\tau}{n} \right) \\
	% term
	+ \frac{C_\tau}{C} H \left( \frac{E_\tau}{C_\tau}
	\right) 
	% term
	+ \frac{C_M}{C} H \left( \frac{E_M}{C_M} \right)
	% term
	+ \frac{C_{\bar \tau}}{C} H \left( \frac{E_{\bar\tau}}{C_{\bar\tau}}\right).
	\label{eq:phinotation}
\end{dmath}
The minus sign is introduced so that this becomes an objective function to 
minimize, as is standard in optimization. The division by 
\(C\coloneqq\binom{n}{2}\) is for normalization purposes. This is a non-convex 
function on \(\tau\), which complicates its optimization.

\begin{comment}
	\subsubsection{Asymptotic objective function}
	
	Observe that the function in \ref{eq:phinotation} is an objective function 
	depending on the graph observed. In particular, it is a function of the 
	size of 
	the graph. It is reasonable to expect that, keeping the underlying model 
	fixed 
	and growing the size \(n\) of the graph sampled from it, we would have an 
	asymptotic objective to which the objective functions \ref{eq:phinotation} 
	converge, and which is independent from any particular observation. Let us 
	calculate this.
	
	As a first simplification, it is easy to see that the first two terms 
	vanish, 
	as \(C \gg n\) when \(n \to \infty\). A second simplification is to perform 
	a 
	change of basis so that the structure of \(A\) and \(J\) becomes 
	simplified. 
	One might consider diagonalizing \(A\), but this does not simplify \(J\). 
	Consider instead the normalized graph Laplacian \(L \coloneqq 
	D^{-\frac{1}{2}} A 
	D^{-\frac{1}{2}}\). We need to know to what vectors its eigenvectors 
	converge. 
	This has been studied in \cite{rohe_spectral_2011} and improved in 
	\cite{deng_strong_2020}.
\end{comment}

\subsection{Expected ELBO as objective function}

Direct maximization of the \(\hat \Phi\) function can be challenging, as it 
is not convex, and its solution might be in the interior of the 
hypercube. However, it is reasonable to expect that the maximum of the ELBO 
\(\mathcal L\) should converge to the maximum of the expected ELBO \(\mathbb E 
[\mathcal L \vert Z]\), where the expectation is taken with respect to the 
randomness of \(A\) and assuming knowledge of the model parameters and \(Z\). 
The expected ELBO should be simpler to treat, and intuitively its maximum 
should be the assignments \(Z\). Numerical simulation supports this intuition.

Thus, proceed in two steps. First, show that the maximum of the expected ELBO 
is indeed the vector of assignments \(Z\). This will be done first on the 
simple case of the SBM with two communities. Then, properly show the 
convergence of the maxima of the ELBO to \(Z\).

\begin{figure}[ht]
	\centering
	% tikz tex files are not taken into account by \graphicspath, indicate
	% images folder explicitly
	\subfile{images/tikz/convergence_tau_z/convergence_tau_z.tex}
	\caption{The maximum \(\hat \tau\) of the ELBO should converge to the 
		maximum of the 
		expected ELBO, which intuitively should be \(Z\).}
	\label{fig:convergence_tau_z}
\end{figure}

Starting from equation \ref{eq:elbo-two-comm}, by linearity it suffices to 
substitute \(A\) by \(\mathscr A \coloneqq \mathbb E \left[A \vert Z\right]\). 
This matrix has a simple structure. If \(J_k \coloneqq \ones[k]^t \ones[k] - 
I_k\), then 
\begin{dmath*}
	\mathscr A = 
	\begin{pmatrix}
		\gamma_{11} J_{n_1} & \gamma_{12} \ones[n_1 \times n_2] \\
		\gamma_{12} \ones[n_2 \times n_1] & \gamma_{22} J_{n_2}
	\end{pmatrix}.
\end{dmath*}
Notice also that \(\bar A \coloneqq J - A\) becomes 
\begin{dmath*}
	J - \mathscr A = 
	\begin{pmatrix}
		(1 - \gamma_{11}) J_{n_1} &  (1-\gamma_{12}) \ones[n_1 \times n_2] \\
		(1-\gamma_{12}) \ones[n_2 \times n_1] & (1-\gamma_{22}) J_{n_2}
	\end{pmatrix},
\end{dmath*}
which is the expected adjacency matrix of complementary graphs to the graphs 
originating from the model. This matrix will be denoted \(\bar{\mathscr{A}} 
\coloneqq  J - \mathscr{A}\). In order to organize the calculations, break the 
ELBO \ref{eq:elbo-two-comm} in two parts:
\begin{equation}
	\mathcal L_{(k=2)} = \mathcal L_{(k=2)}^{\text{log}} + \mathcal 
	L_{(k=2)}^{\text{sym}},
\end{equation}
where
\begin{align*}
	\mathcal L_{(k=2)}^{\text{sym}} &\coloneqq
	% term
	\frac{1}{2} \left( \tau^t A \tau 
	\log{\gamma_{11}} + \tau^t
	\bar A \tau \log{\left( 1 - \gamma_{11} \right)} \right) \\
	% term
	&+ \left( \tau^t A \left( \mathbf{1}_n - \tau \right) \log{\gamma_{12}} 
	+ \tau^t \bar A \left( \mathbf{1}_n - \tau \right) 
	\log{\left( 1 - \gamma_{12} \right)} \right) \\
	% term 
	&+ \frac{1}{2} \left( \left( \mathbf{1}_n - \tau \right)^t A \left(
	\mathbf{1}_n - \tau \right) \log{\gamma_{22}} + \left( \mathbf{1}_n - 
	\tau \right)^t \bar A \left( \mathbf{1}_n - \tau \right) 
	\log{\left( 1 - \gamma_{22} \right)} \right),
\end{align*}
and \(L_{(k=2)}^{\text{log}}\) are the remaining non-random (in \(A\)) terms. 
Denote \(\tau = (\tau_1, \tau_2)\), where \(\tau_1 \in [0,1]^{n_1}\), 
\(\tau_2 \in [0,1]^{n_2}\). Finally, denote \(H(a, b) \coloneqq a \log b + 
(1-a)\log{1-b}\). 
\marginnote{Symmetric term.} The symmetric term above expands to
\begin{dmath*}
	\mathcal L_{(K=2)}^{\text{sym}} =
	% terms
	\frac{1}{2} \left[ 
	H(\gamma_{11}, \gamma_{11}) \tau_1 J_{n_1} \tau_1 
   +H(\gamma_{22}, \gamma_{11}) \tau_2 J_{n_2} \tau_2 
   +2H(\gamma_{12}, \gamma_{11}) \tau_1 
	\ones[n_1 \times n_2] \tau_2 
	\right] \\
	% terms
	+ \left[
	H(\gamma_{11}, \gamma_{12}) \tau_1 J_{n_1} \bar \tau_1
   +H(\gamma_{12}, \gamma_{12}) \tau_1 \ones[n_1 \times n_2] \bar \tau_2
   +H(\gamma_{12}, \gamma_{12}) \tau_2 \ones[n_2 \times n_1] \bar \tau_1
   +H(\gamma_{22}, \gamma_{12})
	\right] \\
	% terms
	+ \frac{1}{2} \left[ 
	H(\gamma_{11}, \gamma_{22}) \bar \tau_1 J_{n_1} \bar \tau_1
	+H(\gamma_{22}, \gamma_{22}) \bar \tau_2 J_{n_2} \bar \tau_2
	+2H(\gamma_{12}, \gamma_{22}) \bar \tau_1 \ones[n_1 \times n_2] 
	\bar \tau_2
	\right].
\end{dmath*}
Now, it holds that \(H(a,b) \leq H(a, a)\) \marginnote{\todo{Put this 
inequality as a proposition !}}, implying (after straightforward 
simplification) that
\begin{dmath*}
	\mathcal L_{(k=2)}^{\text{sym}} \leq C_{n_1} H(\gamma_{11}) + C_{n_2} 
	H(\gamma_{22}) + n_1 n_2 H(\gamma_{12}),
\end{dmath*}
using the notation from \ref{eq:combinatorial-notation}. Notice the right-hand 
side is constant in \(\tau\). Finally, one can check that substituting 
\(\tau^{\star} = (\ones[n_1], \boldmath{0}_{n_2})\) achieves this upper bound. 
Thus it maximizes the symmetric part of the ELBO and corresponds to \(Z\), the 
true community labels.
\marginnote{Logarithmic term.} As for the other term, notice that at 
\(\tau^{\star}\) it becomes
\begin{equation*}
	\mathcal L_{(k=2)}^{\text{log}}(\tau^{\star}) = n_1 \log \pi + n_2 \log{(1 
	- \pi)}.
\end{equation*}
This is not the maximum that such term can reach, since if one takes \(\tau = 
\pi \ones[n]\) then this nonpositive term vanishes. However, notice that in 
order to analyze the ELBO asymptotically, proper normalization is required. 
Dividing the ELBO by \(C = \binom{n}{2}\), this logarithmic term vanishes 
(since it grows linearly on the size of the communities).

\marginnote{Important conclusion.} Therefore, assigning the true labels \(\tau 
= Z\) \textit{asymptotically} maximizes the ELBO \textit{in expectation}. 

\begin{comment}
	The expected ELBO becomes
\begin{dmath*}
	\mathbb E \left[\mathcal{L}\right \vert Z] = \text{log terms} + \\
	\log (p) \left(\frac{1}{2} \tau^t \mathscr A \tau + \tau^t 
	\bar{\mathscr{A}} 
	\bar \tau + \frac{1}{2} \bar{\tau}^t \mathscr A \bar \tau \right) \\
	\log (q) \left(\frac{1}{2} \tau^t \bar{\mathscr{A}} \tau + \tau^t 
	\mathscr{A}
	\bar \tau + \frac{1}{2} \bar{\tau}^t \bar{\mathscr{A}} \bar \tau \right)
\end{dmath*}
Now, in order to develop this calculation, first develop the quantities
\begin{align*}
	\tau^t B \tau &= p \left(n_{\tau_1} - n_{\tau_2}\right)^2 + 2 n_{\tau_1} 
	n_{\tau_2}, \\
	\tau^t B \bar\tau &=  - \left(n_{\tau_1} - n_{\tau_2}\right)^2 + 
	\left(n_{\tau_1} - n_{\tau_2}\right), \\
	\bar{\tau}^t B \bar\tau &= p \left(\left(n_1 - n_2\right) - 
	\left(n_{\tau_1} - n_{\tau_2}\right)\right)^2 + 2 \left(n_1 - 
	n_{\tau_1}\right)\left(n_2 - n_{\tau_2}\right).\\
\end{align*}
\end{comment}

\end{document}
