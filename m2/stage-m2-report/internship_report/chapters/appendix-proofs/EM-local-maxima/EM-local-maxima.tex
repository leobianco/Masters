\documentclass[../../main.tex]{subfiles} % Allows individual compilation
\graphicspath{{\subfix{./images/}}}

\begin{document}

\section{Convergence of VEM to a local 
maximum}
\label{app:em-monotone}
Showing that the algorithm increases the likelihood at each step (and therefore 
achieves some local maximum) is very similar to the case of the classical EM.

\begin{proposition}
	The VEM algorithm increases the likelihood at each step.
\end{proposition}

\begin{proof}
	For any fixed \(\theta_0\), the variational decomposition is
	\begin{equation}
		\log p(x; \theta_0) = F(q_\tau, \theta_0) + \KL (q_\tau \Vert p(\cdot 
		\vert 
		x; \theta_0)).
	\end{equation}
	Observe that in general the \(\KL\) term can no longer be zero, but it can be 
	minimized, leading to the best approximation to the posterior within the 
	mean-field family. Minimizing the \(\KL\) still is equivalent to maximizing 
	the ELBO, thus the new \textit{variational} E step consists in finding 
	\(\tau\), 
	fixed \(\theta_0\):
	\begin{equation}
		\tau_0 = \argmax_\tau F (q_\tau, \theta_0).
	\end{equation}
	Now fix \(\tau_0\) and consider a general \(\theta\) in the variational 
	decomposition:
	\begin{equation*}
		\log p(x; \theta) = F(q_{\tau_0}, \theta) + \KL (q_{\tau_0} \Vert 
		p(\cdot 
		\vert x; \theta)).
	\end{equation*}
	If you take \(\theta_1 \coloneqq \argmax_\theta F (q_{\tau_0}, \theta)\), 
	then
	\begin{align*}
		\log p(x; \theta_1) &= F(q_{\tau_0}, \theta_1) + \KL (q_{\tau_0} \Vert 
		p(\cdot \vert x; \theta_1)) \\
		&\geq F(q_{\tau_0}, \theta_0) + \KL (q_{\tau_0} \Vert p(\cdot \vert x; 
		\theta_0)) \\
		&= \log p(x; \theta_0).
	\end{align*}
	That is, this choice of a next \(\theta\) makes the observed log-likelihood 
	grow. This maximization is the variational analogue of the M step. Notice 
	that the observed log-likelihood does not depend on the \(q_\tau\) chosen, 
	thus the M-step keeps the observed log-likelihood constant, while the E-step 
	makes it grow, and so overall it must grow after each EM alternation. Notice 
	that the ELBO itself grows in both steps.
\end{proof}

However, there can be multiple uninformative local maxima, and it is known 
\cite{sarkar_when_2021} that in closely related algorithms these bad optima can 
attract almost all initializations for the parameters. Consider then the 
question whether this algorithm converges to the \textit{true} global maximum 
as \(n \to \infty\), that is, the question concerning its asymptotic 
consistency.

\end{document}
