\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{2}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Objectives of the internship}{2}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivations for the internship}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Graphs}{2}{subsection.1.2.1}\protected@file@percent }
\newmarginnote{note.5.1}{{6}{21071932sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Graphs with communities}{4}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Common approaches to community detection}{5}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Statistical approaches}{5}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Spectral approaches}{5}{section*.3}\protected@file@percent }
\citation{latouche_overlapping_2011}
\citation{legramanti_extended_2022}
\citation{morelli_nested_2021}
\citation{miele_revealing_2017}
\citation{newman-resources}
\citation{pozo-resources}
\citation{peixoto-resources}
\citation{stanford-resources}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Statistical learning theory}{6}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Applications}{6}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Note}{6}{subsection.1.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Steps taken from observing a graph to estimating its communities. First, either an unknown process or some probabilistic model generates a graph, which is observed, and a community assignment on it, which is hidden (not observed). Then, an algorithm is picked to estimate the communities. This report presents two different approaches, namely spectral and statistical, to develop of such algorithms.\relax }}{7}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:diagram-introduction}{{1.1}{7}{Steps taken from observing a graph to estimating its communities. First, either an unknown process or some probabilistic model generates a graph, which is observed, and a community assignment on it, which is hidden (not observed). Then, an algorithm is picked to estimate the communities. This report presents two different approaches, namely spectral and statistical, to develop of such algorithms.\relax }{figure.caption.4}{}}
\citation{abbe_community_2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Algorithms for community detection}{8}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Statistical approach}{8}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The stochastic block model}{8}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The general SBM}{9}{section*.5}\protected@file@percent }
\newlabel{def:sbm}{{13}{9}{Stochastic blockmodel}{definition.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An example of an SBM graph with assortative communities\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:sbm-example}{{2.1}{9}{An example of an SBM graph with assortative communities\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{The symmetric SBM}{10}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Model likelihood}{10}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Complete model likelihood}{10}{section*.9}\protected@file@percent }
\newlabel{eq:complete-likelihood}{{2.1}{10}{Complete model likelihood}{equation.2.1.1}{}}
\newlabel{fig:bipartite-sbm}{{2.2a}{11}{\relax }{figure.caption.7}{}}
\newlabel{sub@fig:bipartite-sbm}{{a}{11}{\relax }{figure.caption.7}{}}
\newlabel{fig:star-sbm}{{2.2b}{11}{\relax }{figure.caption.7}{}}
\newlabel{sub@fig:star-sbm}{{b}{11}{\relax }{figure.caption.7}{}}
\newlabel{fig:core-periphery-sbm}{{2.2c}{11}{\relax }{figure.caption.7}{}}
\newlabel{sub@fig:core-periphery-sbm}{{c}{11}{\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The SBM is versatile and can give rise to different features, such as (a) bipartite structures, (b) star structures, (c) core-periphery structures.\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:sbm-versatile}{{2.2}{11}{The SBM is versatile and can give rise to different features, such as (a) bipartite structures, (b) star structures, (c) core-periphery structures.\relax }{figure.caption.7}{}}
\citation{koller2009probabilistic}
\citation{koller2009probabilistic}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Graphical model expressing the dependence structure between latent variables \(Z_1, Z_2\) and edge \(A_{12}\); this pattern is commonly called a ``v-structure'' \cite  {koller2009probabilistic}\relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:v-structure}{{2.3}{12}{Graphical model expressing the dependence structure between latent variables \(Z_1, Z_2\) and edge \(A_{12}\); this pattern is commonly called a ``v-structure'' \cite {koller2009probabilistic}\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{Observed model likelihood}{12}{section*.10}\protected@file@percent }
\newlabel{eq:observed-likelihood}{{2.2}{12}{Observed model likelihood}{equation.2.1.2}{}}
\newlabel{eq:dependence-structure}{{2.3}{12}{Observed model likelihood}{equation.2.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Variational decomposition and mean field approximation}{12}{subsection.2.1.3}\protected@file@percent }
\citation{mezard2009information}
\@writefile{toc}{\contentsline {subsubsection}{Deriving the variational decomposition.}{13}{section*.12}\protected@file@percent }
\newlabel{thm:var-decomp}{{1}{13}{}{theorem.2.1.1}{}}
\newlabel{eq:variational-decomposition}{{2.5}{13}{}{equation.2.1.5}{}}
\newmarginnote{note.15.1}{{15}{13718696sp}}
\@writefile{toc}{\contentsline {subsubsection}{Mean field approximation.}{14}{section*.13}\protected@file@percent }
\newmarginnote{note.15.2}{{16}{28456282sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Variational estimation of the SBM}{14}{subsection.2.1.4}\protected@file@percent }
\newlabel{sec:var-estimation-sbm}{{2.1.4}{14}{Variational estimation of the SBM}{subsection.2.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{The ELBO in the SBM case}{14}{section*.14}\protected@file@percent }
\newlabel{eq:mean-field-approx}{{2.7}{14}{The ELBO in the SBM case}{equation.2.1.7}{}}
\newlabel{prop:mf-elbo}{{1}{14}{}{proposition.1}{}}
\newlabel{eq:mf-elbo}{{2.8}{14}{}{proposition.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{The variational EM algorithm}{15}{section*.15}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Variational EM\relax }}{15}{algorithm.1}\protected@file@percent }
\newlabel{alg:vem}{{1}{15}{Variational EM\relax }{algorithm.1}{}}
\newlabel{eq:fixed-point-tau}{{2.9}{15}{The variational EM algorithm}{equation.2.1.9}{}}
\newlabel{eq:pi-hat}{{2.10}{15}{The variational EM algorithm}{equation.2.1.10}{}}
\newlabel{eq:gamma-hat}{{2.11}{15}{The variational EM algorithm}{equation.2.1.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Spectral approach}{16}{section.2.2}\protected@file@percent }
\newlabel{sec:optimization-approach}{{2.2}{16}{Spectral approach}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The spectral point of view}{16}{subsection.2.2.1}\protected@file@percent }
\newmarginnote{note.18.1}{{18}{20078469sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Graph Laplacians}{16}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Defining graph Laplacians}{16}{section*.16}\protected@file@percent }
\citation{von_luxburg_tutorial_2007}
\newlabel{def:laplacian-matrices}{{2.12}{17}{}{equation.2.2.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Laplacian and connectivity}{17}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Laplacian and graph cuts}{17}{section*.18}\protected@file@percent }
\citation{l1996handbook}
\newlabel{eq:balanced-min-cut}{{2.15}{18}{}{equation.2.2.15}{}}
\newlabel{eq:laplacian-balanced-min-cut}{{2.16}{18}{}{equation.2.2.16}{}}
\newlabel{eq:constrain-H}{{2.17}{18}{}{equation.2.2.17}{}}
\newlabel{prop:balanced-min-cut-laplacian}{{3}{18}{}{equation.2.2.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}A spectral clustering algorithm}{18}{subsection.2.2.3}\protected@file@percent }
\citation{von_luxburg_tutorial_2007}
\citation{bui_finding_1992}
\newlabel{eq:relaxed-balanced-min-cut}{{2.18}{19}{}{equation.2.2.18}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Spectral clustering\relax }}{19}{algorithm.2}\protected@file@percent }
\newlabel{alg:spectral-clustering}{{2}{19}{Spectral clustering\relax }{algorithm.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Diagram describing the steps taken to derive spectral algorithms for community detection.\relax }}{20}{figure.caption.19}\protected@file@percent }
\newlabel{fig:diagram-spectral-algorithms}{{2.4}{20}{Diagram describing the steps taken to derive spectral algorithms for community detection.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Statistical learning theory}{21}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Agreement and degrees of recovery}{21}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Agreement}{21}{subsection.3.1.1}\protected@file@percent }
\newlabel{def:agreement}{{21}{21}{Agreement}{definition.21}{}}
\newlabel{eq:def-agreement}{{3.1}{21}{Agreement}{equation.3.1.1}{}}
\citation{erdos59a}
\newlabel{def:normalized-agreement}{{22}{22}{Normalized agreement}{definition.22}{}}
\newlabel{eq:def-normalized-agreement}{{3.2}{22}{Normalized agreement}{equation.3.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Degrees of recovery}{22}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Asymptotic topologies}{22}{section.3.2}\protected@file@percent }
\newlabel{sec:asymptotic-topologies}{{3.2}{22}{Asymptotic topologies}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}The case of the Erd\H {o}s-Rényi model}{23}{subsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An observation from an Erd\H {o}s-Rényi model \(G(30, 0.2)\)\relax }}{23}{figure.caption.20}\protected@file@percent }
\newlabel{fig:obs-erdos-renyi}{{3.1}{23}{An observation from an Erd\H {o}s-Rényi model \(G(30, 0.2)\)\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}The case of the SBM}{23}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Why does this matter?}{23}{subsection.3.2.3}\protected@file@percent }
\newlabel{fig:er-giant-0}{{3.2a}{24}{\relax }{figure.caption.21}{}}
\newlabel{sub@fig:er-giant-0}{{a}{24}{\relax }{figure.caption.21}{}}
\newlabel{fig:er-giant-1}{{3.2b}{24}{\relax }{figure.caption.21}{}}
\newlabel{sub@fig:er-giant-1}{{b}{24}{\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces (a) At \(np = 0.8 < 1\), there are some small trees of size at most \(O(\qopname  \relax o{log}(n))\). (b) At \(np = 1.33 > 1\) a giant component appears, of size \(O(n^{2/3})\).\relax }}{24}{figure.caption.21}\protected@file@percent }
\newlabel{fig:test1}{{3.2}{24}{(a) At \(np = 0.8 < 1\), there are some small trees of size at most \(O(\log (n))\). (b) At \(np = 1.33 > 1\) a giant component appears, of size \(O(n^{2/3})\).\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces At \(p = 0.011 < 0.012\) there exists almost surely an isolated vertex, and the graph is disconnected. When \(p = 0.013 > 0.012\), isolated vertices disappear almost surely, and the graph finally becomes connected.\relax }}{25}{figure.caption.22}\protected@file@percent }
\newlabel{fig:er-connectivity-0}{{3.3}{25}{At \(p = 0.011 < 0.012\) there exists almost surely an isolated vertex, and the graph is disconnected. When \(p = 0.013 > 0.012\), isolated vertices disappear almost surely, and the graph finally becomes connected.\relax }{figure.caption.22}{}}
\citation{rohe_spectral_2011}
\citation{rohe_spectral_2011}
\citation{rohe_spectral_2011}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Consistency of variational EM}{26}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}The case of of spectral clustering}{26}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Motivation}{26}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Notations}{26}{subsection.3.4.2}\protected@file@percent }
\citation{davis-kahan}
\citation{rohe_spectral_2011}
\citation{rohe_spectral_2011}
\citation{rohe_spectral_2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Convergence of eigenvectors}{27}{subsection.3.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Diagram of results in \cite  {rohe_spectral_2011}. First, the question of whether the empirical Laplacians approach the expected version or not is studied; then, the question of whether the expected Laplacian contains or not the complete information on the communities is analyzed.\relax }}{28}{figure.caption.23}\protected@file@percent }
\newlabel{fig:tikz-diagram-rohe}{{3.4}{28}{Diagram of results in \cite {rohe_spectral_2011}. First, the question of whether the empirical Laplacians approach the expected version or not is studied; then, the question of whether the expected Laplacian contains or not the complete information on the communities is analyzed.\relax }{figure.caption.23}{}}
\citation{rohe_spectral_2011}
\citation{rohe_spectral_2011}
\newlabel{prop:convergence-eigenvectors}{{7}{29}{}{equation.3.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Bounding the number of misclassified nodes}{29}{subsection.3.4.4}\protected@file@percent }
\newlabel{eq:equivalence_rohe}{{3.5}{29}{}{equation.3.4.5}{}}
\newlabel{lemma:retrieval-expectation}{{1}{29}{}{equation.3.4.5}{}}
\newlabel{thm:main-rohe}{{3}{30}{}{equation.3.4.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Numerical experiments}{31}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Numerical experiments}{31}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion and outlook}{32}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Conclusions}{32}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Proofs, calculations, extra definitions}{33}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Proof of Theorem \ref  {thm:var-decomp}}{33}{section.A.1}\protected@file@percent }
\newlabel{app:proof-var-decomp}{{A.1}{33}{Proof of Theorem \ref {thm:var-decomp}}{section.A.1}{}}
\citation{mariadassou}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Proof of Proposition \ref  {prop:mf-elbo}}{34}{section.A.2}\protected@file@percent }
\newlabel{sec:proof-mf-elbo}{{A.2}{34}{Proof of Proposition \ref {prop:mf-elbo}}{section.A.2}{}}
\newlabel{eq:rough-elbo}{{A.1}{34}{Proof of Proposition \ref {prop:mf-elbo}}{section.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Proof of EM explicit steps}{35}{section.A.3}\protected@file@percent }
\newlabel{proof:em-steps}{{A.3}{35}{Proof of EM explicit steps}{section.A.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Optimizing for \(\hat  \tau \)}{36}{section*.24}\protected@file@percent }
\newlabel{eq:fixed-point-tau-proof}{{A.2}{36}{Optimizing for \(\hat \tau \)}{equation.A.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Optimizing for \(\hat  \theta \)}{36}{section*.25}\protected@file@percent }
\newlabel{eq:pi-hat}{{A.3}{37}{Optimizing for \(\hat \theta \)}{equation.A.3.3}{}}
\newlabel{eq:gamma-hat-proof}{{A.4}{37}{Optimizing for \(\hat \theta \)}{equation.A.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Convergence of VEM to a local maximum}{37}{section.A.4}\protected@file@percent }
\newlabel{app:em-monotone}{{A.4}{37}{Convergence of VEM to a local maximum}{section.A.4}{}}
\citation{sarkar_when_2021}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Sketch of proof of Lemma \ref  {lemma:retrieval-expectation}}{38}{section.A.5}\protected@file@percent }
\newlabel{appendix:sketch-lemma-retrieval}{{A.5}{38}{Sketch of proof of Lemma \ref {lemma:retrieval-expectation}}{section.A.5}{}}
\newlabel{eq:proof-rohe}{{A.7}{38}{Sketch of proof of Lemma \ref {lemma:retrieval-expectation}}{equation.A.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Asymptotic notation}{39}{section.A.6}\protected@file@percent }
\newlabel{app:asymptotic-notation}{{A.6}{39}{Asymptotic notation}{section.A.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}The case of two communities}{40}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}The case of two communities*}{40}{section.B.1}\protected@file@percent }
\newlabel{sec:two-communities}{{B.1}{40}{The case of two communities*}{section.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.1}Rewriting the ELBO}{40}{subsection.B.1.1}\protected@file@percent }
\newlabel{eq:elbo-two-comm}{{B.1}{41}{Rewriting the ELBO}{subsection.B.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.2}The \(\Phi \) function}{41}{subsection.B.1.2}\protected@file@percent }
\newlabel{eq:pi-hat-two}{{B.2}{41}{The \(\Phi \) function}{equation.B.1.2}{}}
\newlabel{eq:phi}{{B.6}{42}{The \(\Phi \) function}{equation.B.1.5}{}}
\newlabel{eq:combinatorial-notation}{{B.7}{42}{The \(\Phi \) function}{equation.B.1.7}{}}
\newlabel{eq:phinotation}{{B.9}{42}{The \(\Phi \) function}{equation.B.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.3}Expected ELBO as objective function}{43}{subsection.B.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces The maximum \(\hat  \tau \) of the ELBO should converge to the maximum of the expected ELBO, which intuitively should be \(Z\).\relax }}{43}{figure.caption.26}\protected@file@percent }
\newlabel{fig:convergence_tau_z}{{B.1}{43}{The maximum \(\hat \tau \) of the ELBO should converge to the maximum of the expected ELBO, which intuitively should be \(Z\).\relax }{figure.caption.26}{}}
\newmarginnote{note.46.1}{{46}{8275230sp}}
\newmarginnote{note.46.2}{{46}{18974264sp}}
\newmarginnote{note.46.3}{{46}{31139808sp}}
\newmarginnote{note.46.4}{{46}{8275230sp}}
\citation{*}
\bibstyle{plain}
\bibdata{references}
\bibcite{abbe_community_2017}{1}
\bibcite{bui_finding_1992}{2}
\bibcite{deng_strong_2020}{3}
\bibcite{erdos59a}{4}
\bibcite{koller2009probabilistic}{5}
\bibcite{latouche_overlapping_2011}{6}
\bibcite{legramanti_extended_2022}{7}
\bibcite{stanford-resources}{8}
\bibcite{l1996handbook}{9}
\bibcite{mariadassou}{10}
\bibcite{mezard2009information}{11}
\bibcite{miele_revealing_2017}{12}
\bibcite{morelli_nested_2021}{13}
\bibcite{newman-resources}{14}
\bibcite{peixoto-resources}{15}
\bibcite{pozo-resources}{16}
\bibcite{rohe_spectral_2011}{17}
\bibcite{sarkar_when_2021}{18}
\bibcite{van2000asymptotic}{19}
\bibcite{von_luxburg_tutorial_2007}{20}
\bibcite{davis-kahan}{21}
\gdef \@abspage@last{47}
