\documentclass[../../main.tex]{subfiles} % Allows individual compilation
\graphicspath{{\subfix{./images/}}}

\begin{document}

\section{Statistical approach}

In this section, the general framework for 
\textit{statistical} approaches to community 
detection is laid down. It consists in assuming a 
model for the observed graph, then estimating the 
parameters of the model by approximately 
maximizing the likelihood using an algorithm similar 
to the classical EM, called the \textit{variational EM 
algorithm}. The communities can then immediately 
be inferred from the variational parameters 
optimized during the inference procedure. Here, the 
model chosen is the SBM due to its popularity.

\subsection{Model likelihood}

\subsubsection{Complete model likelihood}

The community assignment variable \(Z\) is latent, in the sense that in 
practice it is not observed. However, it \textit{is} a random variable of the 
model that is determined at the moment of sampling of the graph observed, and 
thus different possible assignments have different probabilities associated to 
them. What is called the \textit{complete} model likelihood is the likelihood 
taking \(Z\) into account as a variable. It writes
\begin{equation} \label{eq:complete-likelihood}
	p(A, Z) = \prod_{i=1}^{n} \pi_{Z_i} \prod_{\substack{i=1 \\ j>i}}^n 
	\gamma_{Z_i 
		Z_j}^{A_{ij}} (1 - \gamma_{Z_i Z_j})^{1 - A_{ij}}.
\end{equation}
Of course it cannot be calculated from an observation, since the \(Z\) are 
unknown.

\begin{remark}
	Equation \eqref{eq:complete-likelihood} is an example of the abuse of 
	notation described in the remarks below Definition \ref{def:sbm}.
\end{remark}

\subsubsection{Observed model likelihood}

In order to have a likelihood associated to an observation, one needs to 
marginalize the latent variables present in Equation 
\eqref{eq:complete-likelihood}. That is,
\begin{equation} \label{eq:observed-likelihood}
	p(A) \coloneqq \sum_{Z \in K^n} p(A, Z).
\end{equation}
This sum over the whole latent space is intractable, since it has an 
exponential number of terms and it cannot be analytically evaluated to an 
useful simpler form. Therefore, it will be strictly necessary to approximate 
this observed likelihood in order to perform estimation and inference on SBMs 
from a probabilistic point of view.

\subsection{Variational decomposition and mean 
field approximation}
The likelihood of an observation under the SBM, in Equation 
\eqref{eq:observed-likelihood}, is complex to deal with for mainly two reasons. 
First, it is a sum over all latent configurations, and thus it has an 
exponential number of terms, making it intractable. Second, it can have 
multiple local optima. Therefore, approximations are needed in order to work 
with this model. A common one is the variational approximation to the 
likelihood, which will deal with the problem of summing an exponential number 
of terms. This is still complicated in all its generality, so a second ``mean 
field'' approximation is used on top of the first variational one. This 
consists in searching the solution to the variational approximation amidst 
factorizable distributions.

\subsubsection{Deriving the variational 
decomposition.}
Let \(Z\) denote the vector of latent variables, \(A\) an 
observation, and \(\theta\) the parameters of an SBM. The following 
\textit{variational decomposition} leads to an useful approximation to the 
likelihood.

\begin{definition}
	For any two distributions \(p(z), q(z)\),
	\begin{equation}
		\KL (q \Vert p) \coloneqq -
		\int_{Z} \log \left( \frac{p (z)}{q(z)}
		\right) q(z) \, dz
	\end{equation}
	is called the \textit{Kullback-Leibler divergence} from \(p(z)\) to 
	\(q(z)\).
\end{definition}

\begin{theorem} \label{thm:var-decomp}
	The observed likelihood can be decomposed as
	\begin{equation}
		\log p (A; \theta) = F (q, \theta) + \KL (q \Vert p( Z \vert 
		A; \theta)),
		\label{eq:variational-decomposition}
	\end{equation}
	where
	\begin{equation}
		F (q, \theta) \coloneqq \int_{Z} \log \left(
		\frac{p \left( Z, A; \theta \right)}{q(Z)} \right) q(Z) \, dZ
	\end{equation}
	is called the \textit{evidence lower bound}, or ELBO for short.
\end{theorem}

For the proof, see Section \ref{app:proof-var-decomp}. Equation 
\eqref{eq:variational-decomposition} forms the basis of the classical 
EM algorithm for estimating \(\theta\), where one performs alternate 
minimization of the KL term and subsequent maximization of the term 
\(F \left( q, \theta \right)\). 
A classical result proves that the KL is always positive. Therefore, the ELBO 
is indeed a lower bound for the log likelihood being decomposed. 
\marginnote{The ELBO is also called the ``free 
energy'' in the literature.} 
Given the independence of the left hand side with respect to $q$, observe that 
maximizing the ELBO amounts to minimizing the KL term, and this can be done by 
setting \(q = \condp{Z}{A; \theta}\). However, in the case of the SBM, this 
conditional probability is itself intractable, therefore this step of EM must 
be performed differently.

\subsubsection{Mean field approximation.} A 
common strategy used to deal with the 
problem of having an untractable solution \(q\) to the variational 
approximation is called the ``mean field approximation''. \marginnote{There are 
other ``correlated'' mean field approximations, see 
\cite{mezard2009information}.}[-2ex] It consists in trying to find a \(q\) 
distribution maximizing the ELBO constrained to a family of tractable 
distributions. Here, tractability means factorizability: consider distributions 
of the form
\begin{equation*}
	q \left( Z \right) = \prod_{i=1}^{n} q_i \left( Z_i \right).
\end{equation*}

\subsection{Variational estimation of the SBM} 
\label{sec:var-estimation-sbm}
\subsubsection{The ELBO in the SBM case}
In the case of the SBM, each factor in the mean field approximation must be 
multinomial distribution, and they differ only by their parameters, that is,
\begin{equation}
	q \left( Z \right) = \prod_{i=1}^n m \left( Z_i; \tau_i \right),
	\label{eq:mean-field-approx}
\end{equation}
where \(m(\cdot, \tau_i)\) is the probability mass function of a multinomial
distribution with parameter \(\tau_i\). It is then possible to find an explicit 
form for the ELBO of an observation assuming the SBM model.

\begin{proposition} \label{prop:mf-elbo}
	Given an adjacency matrix \(A\) and assuming as \SBM{n}{\pi}{\Gamma}, the 
	mean-field ELBO is given by
	\begin{dmath}
		F_A (\tau, \pi, \Gamma) = \sum_{i=1}^n \sum_{k=1}^K \left[\tau_{ik}
		\log{\frac{\pi_k}{\tau_{ik}}} + \frac{1}{2} \sum_{j=1}^n \sum_{l=1}^K
		\tau_{ik} \tau_{jl} \left( A_{ij} \log{\gamma_{kl}} + \left( 1 -
		\delta_{ij} - A_{ij}
		\right) \log{\left( 1 - \gamma_{kl} \right)} \right) \right].
		\label{eq:mf-elbo}
	\end{dmath}
\end{proposition}

For the proof, see Section \ref{sec:proof-mf-elbo}.

\subsubsection{The variational EM algorithm}
Traditionally, numerically maximizing the ELBO in models with latent variables 
is done via the EM algorithm. In this case, since there is the extra step of 
approximating the \(q\) distribution within the mean-field variational family, 
the algorithm is called the \textit{variational EM}. Its steps are
\begin{align*}
	\tau_{t+1} &\coloneqq \argmax_\tau F(q_\tau, \theta_{t}) \quad &\text{(E 
		step)} \\
	\theta_{t+1} &\coloneqq \argmax_\theta \mathbb E_{q_{\tau_{t+1}}} [\log p 
	(x, z; \theta_{t})] \quad &\text{(M step)}.
\end{align*}
One performs this iteratively until convergence or some stopping 
criterion. 

It is possible to explicitly describe these steps. The E step can be calculated 
by solving the fixed point relation
\begin{equation}
	\hat \tau_{ik} \propto \pi_k \prod_{\substack{j > i \\ l = 1, \dots, K}}
	\left( \gamma_{kl}^{A_{ij}} \left( 1 - \gamma_{kl} \right)^{\left( 1 -
		A_{ij}
		\right)} \right)^{\tau_{jl}}.
	\label{eq:fixed-point-tau}
\end{equation}
The M step can be calculated directly by
\begin{equation} \label{eq:gamma-hat-proof}
	\hat \gamma_{kl} = \frac{\sum_{i=1}^n \sum_{j=1}^n \tau_{ik}
		\tau_{jl} A_{ij}}{\sum_{i=1}^n \sum_{j=1}^n \tau_{ik} \tau_{jl} \left(
		1 - \delta_{ij} \right)}.
\end{equation}
For a proof of these statements, see Section \ref{proof:em-steps}.

\end{document}
