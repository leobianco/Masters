\documentclass[../../main.tex]{subfiles} % Allows individual compilation
\graphicspath{{\subfix{./images/}}}

\begin{document}
\section{Optimization approach}  % ðŸ“
\subsection{The spectral point of view}  % âœ…
The use of transformations has proven to be, time and again, very useful in 
mathematics. Classical examples are the Fourier transform and its uses in 
signal processing, and the Laplace transform with its uses in differential 
equations and probability. The idea is always to bring a dual point of view to 
the problem. For example, the Fourier transform allows one to analyze a signal 
processing problem in the time domain or in the frequency domain, according 
to which one is more convenient.

The same idea applies for graphs. On the one hand, a graph can be analyzed in 
the \textit{topological} domain, and quantities based on the nodes' connectivity 
can be calculated. Examples of topological quantities are the diameter, the 
degree, the clustering coefficient, the girth, but there are many others. On the 
other hand, the graph can be represented by a symmetric adjacency matrix 
\(A\) that is completely characterized by its eigensystem. This follows from the 
spectral decomposition \(A = X \Lambda X^t\).

There are reasons for which the spectral domain might present advantages for 
analyzing a graph. The topological quantities described are usually correlated 
and dependent, while the eigenvectors of \(A\) are orthogonal and its 
eigenvalues are independent quantities. The spectral domain representation of 
a graph lies in an Euclidean space, so if structures appear in it it is possible to 
use classical algorithms on such spaces. These tend to be simpler, faster, and 
more interpretable. This is precisely the strategy of spectral clustering.

\subsection{Graph Laplacians}  % ðŸ“
\subsubsection{Defining graph Laplacians}  % âœ…
Although the adjacency matrix completely describes the graph, other matrices 
(or ``\textit{graph operators}''), can be used for constructing spectral 
representations of \(G\). One popular class of matrices used are the different 
graph Laplacians. The unnormalized and normalized Laplacians are of particular 
interest.
\begin{definition}
	Let \(G\) be a simple unweighted graph. Denote by \(A\) its adjacency 
	matrix. Define the \textit{degree matrix} as the diagonal matrix \(D\) such 
	that \(D_{ii} \coloneqq \sum_{ij} A_{ij}\) for each \(i \in \{1, \dots, n\}\). 
	Define the unnormalized and normalized Laplacians respectively by
	\begin{equation}
		\begin{aligned}
			L_{\text{unn}} &\coloneqq D - A \\
			L_{\text{sym}} &\coloneqq I - D^{-1/2} A D^{-1/2}.
		\end{aligned}
	\end{equation}
\end{definition}
There are several different ways of motivating this popularity. In what follows, 
this is explained from a community detection perspective, in the intuitive case 
of assortative communities. 

\subsubsection{The Laplacian and connectivity}  % âœ…
Assume a disconnected graph \(G\) having \(k\) connected components. A 
particular instance of this is when there are \(k\) assortative communities that 
are completely separated. The kernel of the Laplacian contains precisely the 
connectivity information of the graph, and in this degenerate case this 
coincides with the community information.
\begin{proposition}
	Let \(G\) be a simple unweighted graph with \(k\) connected components 
	\(\Omega_1, \dots, \Omega_k\). Then the algebraic multiplicity of the 
	eigenvalue \(0\) of \(L_{\text{unn}}\) equals \(k\) and the indicator vectors 
	\(\mathbf{1}_{\Omega_1}, \dots, \mathbf{1}_{\Omega_k}\) span its null 
	space.
\end{proposition}
\begin{remark}
An analogous proposition holds for \(L_{\text{sym}}\), with the sole difference 
being that it is the vectors \(\{D^{1/2} \mathbf{1}_{\Omega_i}\}_{i = 1, \dots, 
n}\) that span the null space of the Laplacian.
\end{remark}

\subsubsection{The Laplacian and graph cuts}  % 
Moving on from the degenerate case, consider now that the graph is perturbed 
and the \(k\) communities now communicate weakly, that is, that there are 
some edges across them. Then it makes sense to propose a method of finding 
the communities by seeking a partition which minimizes the number of edges 
crossing classes, while still keeping the partitions with a reasonable size to 
avoid degenerate solutions. Formalizing this yields the following definitions.
\begin{definition}  % Cut
	Let \(G = (V, E)\) be a simple graph. The \textit{cut} is a function associating 
	any partition \(P\) of \(G\) to the number of edges connecting nodes 
	belonging to different classes, i.e.,
	\begin{equation}
		\text{cut}(P) \coloneqq \frac{1}{2} \sum_{i=1}^n  \sum_{j: P(i) \neq P(j)} 
		A_{ij}.
	\end{equation}
\end{definition}
The first step is to normalize this metric with respect to class sizes, to avoid 
taking unbalanced solutions. The \textit{ratio cut} is a possible normalization of 
the cut. The results that follow will be associate it to the unnormalized 
Laplacian. Taking the alternative \(\textup{NCut}\) normalization yields their 
analogous version for the symmetric normalized Laplacian. For the sake of 
simplicity, only the results using the ratio cut will be presented.
\begin{definition}  % Ratio cut
	Let \(\text{cut}(P)_{ij}\) denote the cut between classes \(i\) and \(j\) of 
	partition \(P\), assumed to have \(k\) classes. The ratio cut is defined as
	\begin{equation}
		\text{RatioCut}_k (P) \coloneqq \frac{1}{2} \sum_{i=1}^k \vert P_i 
		\vert^{-1} \sum_{\substack{j=1 \\ j \neq i}}^k \text{cut}(P)_{ij}.
	\end{equation}
\end{definition}
\begin{definition}  % Balanced min-cut problem
	The \textit{balanced min-cut problem} is the following optimization problem:
	\begin{equation} \label{eq:balanced-min-cut}
		\min_{P \in \mathcal P_k(G)} \text{RatioCut}_k(P),
	\end{equation} 
	where \(\mathcal P_k(G)\) denotes the set of all partitions of \(G\) into \(k\) 
	classes.
\end{definition}
This problem can be rewritten in terms of the Laplacian.
\begin{proposition}  % Laplacian Balanced min-cut problem
The balanced min-cut problem can be rewritten in terms of the Laplacian as
\begin{equation}
	\begin{aligned}
	    &\min_{A_1, \dots, A_k} \textup{Tr} (H^t L H) \\
	    &\text{s.t. } H^t H = I,
	\end{aligned}
	\label{eq:laplacian-balanced-min-cut}
\end{equation}
where \(H \in \mathbb R^{n \times k}\) is the matrix
\begin{equation}
	h_{ij} \coloneqq \begin{cases}
		1/\sqrt{\vert A_{ij} \vert} \quad \text{if node } i \in A_j \\
		0 \quad \text{otherwise.}  
	\end{cases}
\label{eq:constrain-H}
\end{equation}
\label{prop:balanced-min-cut-laplacian}
\end{proposition}

\subsection{The spectral clustering algorithm}  % ðŸ“
The problem in Proposition \ref{prop:balanced-min-cut-laplacian} is NP-hard 
due to its hard constraint \eqref{eq:constrain-H} on the form of the matrix 
\(H\). It is natural to drop this constraint to get a solvable approximation to the 
problem.
\begin{definition}  % Relaxed balanced min-cut problem
The \textit{relaxed balanced min-cut problem} is defined by replacing the 
constraint \ref{eq:constrain-H} on the form of \(H\) by a more general 
orthogonality constraint:
	\begin{equation}
		\begin{aligned}
			&\min_{H \in \mathbb R^{n \times k}} \textup{Tr} (H^t L H) \\
			&\text{s.t. } H^t H = I
		\end{aligned}
		\label{eq:relaxed-balanced-min-cut}
	\end{equation}
\end{definition}
This kind of problem has a known solution, given by the Rayleigh-Ritz theorem.
\begin{proposition}[Rayleigh-Ritz]
	Problem \eqref{eq:relaxed-balanced-min-cut} is solved by the matrix \(H\) 
	having the first \(k\) eigenvectors of \(L\) as columns.
\end{proposition}

% Enhance this key paragraph  ðŸŒ«
Originally, \(H\)'s columns indicated the communities, by constraint 
\eqref{eq:constrain-H}. One might expect that this solution to the relaxed 
problem might still contain this information. Therefore the final step is to go 
from this approximate solution to community assignments. Originally, the fact 
that \(H\)'s columns were indicatrices for the communities means that there 
were only \(k\) distinct rows in it. That is, by seeing the rows of \(H\) as 
vectors, there were only \(k\) distinct vectors. One might expect that, seeing 
the rows of the approximate solution \(H_{\text{approx}}\) as vectors, these 
vectors still ``fluctuate'' around the original \(k\) distinct vectors. If this is the 
case, clustering these vectors might yield the community information. This 
clustering can be done by \(k\)-means, for example. This procedure is what is 
commonly called the spectral clustering method. \todo{Enhance this paragraph.}

% Comment here about the need of transforming the eigenvectors into 
% community information: rounding vs. clustering.

% WRITE HERE THE SPECTRAL CLUSTERING ALGORITHM

% Diagram:
% Balanced Min Cut => Laplacian (NP-hard) => Relaxation (Rayleigh-Ritz)
% => Transforming eigenvectors into community information (k-means).

\begin{remark}
	Spectral clustering \textit{tries} to find a balanced minimum cut partition. It 
	may succeed in applications, but theoretically there are simple 
	counterexamples showing that the quality of this approximation can be 
	arbitrarily bad, see \cite{von_luxburg_tutorial_2007}. Searching other 
	algorithms can only partially help, as there is no general efficient algorithm 
	for solving graph cut problems \todo{cite}.
\end{remark}

Long story short, here are the steps of the spectral clustering algorithm.
\todo{Write the algorithm}.
\begin{verbatim}
Step 1: Calculate the Laplacian L from the adjacency matrix A;
Step 2: Calculate the k first eigenvectors of L, make them columns
            of a matrix H;
Step 3: Cluster the rows of H with an algorithm such as k-means;
Step 4: Return cluster assignments as community assignments for
            the nodes;
\end{verbatim}

The diagram below illustrates the intuition and logic steps behind the deduction 
of spectral clustering algorithms.
\begin{figure}[ht]
	\centering
	\ifthenelse{\boolean{addtikz}}{
		\begin{adjustbox}{center}
			\resizebox{1.25\textwidth}{!}{
				\subfile{./images/tikz/diagram-spectral-algorithms/diagram-spectral-algorithms.tex}
			}
		\end{adjustbox}
	}{
		\includegraphics{example-image-a}
	}
	\caption{Diagram describing the steps taken to derive spectral algorithms for 
	community detection.}
	\label{fig:diagram-spectral-algorithms}
\end{figure}


\end{document}
